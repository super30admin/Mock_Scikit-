# -*- coding: utf-8 -*-
"""mock_scikit_solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c0zuONSWcduSTvkBsz3uImB0cmCc1XU9
"""

"""Scikit Learn Mock Interview Solution
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1htZlQ5uZ-E3Zj2u9-DSb9iQJvQIlLTxQ
As a data scientist at a telecommunications company, you have been tasked with developing a classification model to predict customer churn (cancellation of subscriptions) based on their historical behavior and demographic information. The company wants to understand which customers are likely to churn in order to develop targeted customer retention programs.
**Dataset:**
You have been provided with a dataset named "Telco-Customer-Churn.csv". Each row in the dataset represents a customer, and each column contains attributes related to the customer's services, account information, and demographic details. The dataset includes the following information:
1.   Customers who left within the last month (Churn)
2.   Services that each customer has signed up for (phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies)
3.   Customer account information (how long they've been a customer, contract type, payment method, paperless billing, monthly charges, and total charges)
4.   Demographic information about customers (gender, age range, and whether they have partners and dependents)
**Objective:**
Your task is to develop a classification model that can accurately predict whether a customer is likely to churn based on the available information. The model should be trained on a portion of the dataset and evaluated on the remaining unseen data.
**Approach:**
1.   Load and preprocess the dataset: Load the dataset and handle any missing values. Encode categorical variables using appropriate encoding techniques.
2.   Split the dataset: Split the dataset into training and testing sets to train the model on a subset of data and evaluate its performance on unseen data.
3.   Feature scaling: Perform feature scaling on the numerical features to ensure they are on a similar scale, using techniques like StandardScaler.
4.   Model selection and training: Train multiple classification models such as K-Nearest Neighbors, Random Forest, Support Vector Machines, and Logistic Regression.
5.   Model evaluation: Evaluate the trained models on the testing set using appropriate evaluation metrics such as accuracy.
6.   Select the best model: Compare the performance of different models and select the best-performing model based on the chosen evaluation metric.
7.   Predict churn: Use the selected model to predict churn for new, unseen data.
**Deliverables:**
1.   Preprocessed dataset without missing values.
2.   Trained classification models with their respective evaluation metrics.
3.   The best-performing model for churn prediction.
4.   Predictions of churn for new, unseen data.
**Note:** The final model can be used by the telecommunications company to identify customers who are likely to churn, enabling them to take proactive measures to retain those customers.
Remember to adapt the problem statement and approach as needed, based on any specific requirements or modifications provided by the interviewer or the organization you are applying to.
"""
import sys
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
# load the dataset
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/s30/Telco-Customer-Churn (3).csv').drop(columns = ['customerID'])

#Checking for null values
data.isnull().any()

data.dtypes

data.head()

label_encoder = LabelEncoder()
for c in data.columns:
  if data.dtypes[c] == 'object':
    data[c] = label_encoder.fit_transform(data[c])

data.head()

X = data.drop(['Churn', 'MultipleLines', 'InternetService', 'TotalCharges'], axis=1)
y = data['Churn']
# Split the dataset into training and testing sets to train the model on a subset of data and evaluate its performance on unseen data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# let's implement feature scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_test

# Training of multiple classification models such as K-Nearest Neighbors, Random Forest, Support Vector Machines, and Logistic Regression.
knn = KNeighborsClassifier(n_neighbors = 3)
rfc = RandomForestClassifier(n_estimators = 7, criterion = 'entropy', random_state = 7)
svc = SVC()
lr = LogisticRegression()
max_value = -sys.maxsize - 1
bestClassificationModel = ""
# Comparing the performance of different models and selecting the best-performing model based on the chosen evaluation metric.
for classifier in (knn, rfc, svc, lr):
  classifier.fit(X_train, y_train)
  y_pred = classifier.predict(X_test)
  print("Accuracy score of ",classifier.__class__.__name__,"=",
          100*metrics.accuracy_score(y_test, y_pred))
  if 100*metrics.accuracy_score(y_test, y_pred) > max_value:
    max_value = 100*metrics.accuracy_score(y_test, y_pred)
    bestClassificationModel = classifier.__class__.__name__
print(max_value)
print(bestClassificationModel)

# Training of multiple classification models such as K-Nearest Neighbors, Random Forest, Support Vector Machines, and Logistic Regression.
knn = KNeighborsClassifier(n_neighbors = 3)
rfc = RandomForestClassifier(n_estimators = 7, criterion = 'entropy', random_state = 7)
svc = SVC()
lr = LogisticRegression()
models = [knn, rfc, svc, lr]
accuracy = []
for model in models:
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  accuracy.append(metrics.accuracy_score(y_test, y_pred)*100)
print(accuracy)
print("best model", models[accuracy.argmax])